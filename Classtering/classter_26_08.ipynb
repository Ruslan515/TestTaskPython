{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластеризация товаров по их описанию(только ИМпорт)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные храняться в файле 3208.xlsx. Само описание в G31_1_description = 'G31_1 (Наименование и характеристики товаров)'. Для кластеризации используеться метод определения TF-IDF векторов для текста описания товара. \n",
    "Перед векторизацией данные группируются по описанию и удалаються все знаки препинания. В процессе векторизации пременяеться токенизация и стемминг.\n",
    "После получения ветора для кластеризации применяеться метод К-средних(со случайно выбранным числом кластеров - 25) и \n",
    "метод DBSCAN. Стоп-слова по русскому языку храняться в файле \"stopwords-ru.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'becaus', 'befor', 'could', 'doe', 'dure', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'would', 'yourselv', 'алл', 'бел', 'близк', 'бол', 'больш', 'буд', 'будеш', 'будт', 'быв', 'быва', 'быт', 'важн', 'вдал', 'ве', 'вед', 'везд', 'вернут', 'взят', 'видет', 'вмест', 'вод', 'войн', 'вообщ', 'восем', 'восемнадца', 'восемнадцат', 'восьм', 'впроч', 'врем', 'времен', 'всег', 'всегд', 'всюд', 'втор', 'выйт', 'главн', 'говор', 'голов', 'дава', 'давн', 'даж', 'далек', 'дальш', 'дар', 'дат', 'двадца', 'двадцат', 'двенадца', 'двенадцат', 'двер', 'девя', 'девят', 'девятнадца', 'девятнадцат', 'действительн', 'дела', 'ден', 'деньг', 'деся', 'десят', 'довольн', 'долг', 'долж', 'должн', 'дорог', 'дума', 'душ', 'есл', 'ест', 'ждат', 'жен', 'женщин', 'жизн', 'жит', 'зан', 'зат', 'зач', 'зде', 'земл', 'знат', 'знач', 'идт', 'имен', 'имет', 'иногд', 'кажд', 'кажет', 'каза', 'книг', 'ког', 'когд', 'комнат', 'конечн', 'котор', 'кром', 'круг', 'куд', 'лежа', 'лиц', 'лиш', 'лучш', 'люб', 'люд', 'мал', 'маленьк', 'мат', 'машин', 'межд', 'мел', 'мен', 'меньш', 'мест', 'миллион', 'мим', 'минут', 'мно', 'мног', 'многочислен', 'можн', 'можх', 'москв', 'моч', 'наверх', 'наибол', 'найт', 'нача', 'нег', 'недавн', 'недалек', 'некотор', 'нельз', 'немн', 'непрерывн', 'нередк', 'нескольк', 'нибуд', 'ниж', 'низк', 'никак', 'никогд', 'никт', 'никуд', 'нич', 'ничт', 'нов', 'ног', 'ноч', 'нужн', 'обычн', 'одиннадца', 'одиннадцат', 'одн', 'однажд', 'однак', 'оказа', 'окн', 'окол', 'опя', 'особен', 'оста', 'ответ', 'откуд', 'отовсюд', 'отсюд', 'очен', 'перв', 'писа', 'плеч', 'подойд', 'подума', 'пожалуйст', 'позж', 'пойт', 'пок', 'получ', 'помн', 'понима', 'поня', 'посл', 'последн', 'посмотрет', 'посред', 'пот', 'поч', 'почт', 'правд', 'прекрасн', 'прост', 'прот', 'процент', 'пут', 'пят', 'пятнадца', 'пятнадцат', 'работ', 'разв', 'ран', 'раньш', 'реш', 'росс', 'рук', 'русск', 'сво', 'сдела', 'сеа', 'себ', 'сегодн', 'седьм', 'сем', 'семнадца', 'семнадцат', 'сидет', 'сил', 'сказа', 'скольк', 'слишк', 'слов', 'случа', 'смотрет', 'снача', 'снов', 'соб', 'советск', 'совс', 'спасиб', 'спрос', 'сраз', 'стар', 'стат', 'сторон', 'стоя', 'стран', 'сут', 'счита', 'такж', 'тво', 'теб', 'тепер', 'тоб', 'тог', 'тогд', 'тож', 'тольк', 'трет', 'тринадца', 'тринадцат', 'туд', 'увидет', 'улиц', 'умет', 'утр', 'хорош', 'хот', 'хотел', 'хотет', 'хочеш', 'част', 'чащ', 'чег', 'четверт', 'четыр', 'четырнадца', 'четырнадцат', 'чут', 'шест', 'шестнадца', 'шестнадцат', 'эт', 'явля'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "G31_1_description = 'G31_1 (Наименование и характеристики товаров)'\n",
    "G011_direction = 'G011 (Направление перемещения)' #импорт - экспорт\n",
    "G022_name_sender = 'G022 (Наименование отправителя)'\n",
    "G080_recipient_OGRN = 'G080 (ОГРН получателя)'\n",
    "G081_recipient_INN = 'G081 (ИНН получателя)'\n",
    "G082_recipient_Name = 'G082 (Наименование получателя)'\n",
    "G221_currency = 'G221 (Букв.код валюты контракта)'\n",
    "G35_gross_weight = 'G35 (Вес брутто, кг)'\n",
    "G38_net_weight = 'G38 (Вес нетто, кг)'\n",
    "G12_customs_value_total = 'G12 (Общая таможенная стоимость по ГТД)'\n",
    "G222_invoice_value_total = 'G222 (Общая фактурная стоимость по ГТД)'\n",
    "G23_exchange_rates = 'G23 (Курс валюты)'\n",
    "G42_invoice_value = 'G42 (Фактурная стоимость)'\n",
    "G42RUB_invoice_value_rub = 'G42RUB (Фактурная стоимость в рублях)'\n",
    "G45_customs_value = 'G45 (Таможенная стоимость)'\n",
    "G46_statistic_value = 'G46 (Статистическая стоимость, USD.)'\n",
    "usd_kg = 'USDKG (USD за КГ)'\n",
    "G30_city_warehouse = 'G30CITY (Город склада)'\n",
    "\n",
    "#нужные нам стольбцы\n",
    "columns = [G31_1_description, G022_name_sender, G080_recipient_OGRN, G081_recipient_INN, \n",
    "           G082_recipient_Name, G221_currency, G23_exchange_rates, \n",
    "           G30_city_warehouse, G38_net_weight, G42_invoice_value, G42RUB_invoice_value_rub, G46_statistic_value, usd_kg]\n",
    "\n",
    "#функция токенизации и стемминга\n",
    "def token_and_stem(text):\n",
    "    tokens  = [word for word in nltk.word_tokenize(text)] # выделяем слова\n",
    "    #print(tokens, '\\n')\n",
    "    filt_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[а-яА-Яa-zA-Z]', token): #убираем цифры\n",
    "            if len(token) > 3: #смотрим, что бы слова состояли минимум из 4 символов\n",
    "                filt_tokens.append(token)\n",
    "    #print(filt_tokens, '\\n')\n",
    "    stems = [] #стемминг\n",
    "    for token in filt_tokens:\n",
    "        if re.search('[а-яА-Я]', token): #для русских слов стемминг\n",
    "            stems.append(stemmer_rus.stem(token))\n",
    "        elif re.search('[a-zA-Z]', token):#для английских слов стемминг\n",
    "            stems.append(stemmer_eng.stem(token))\n",
    "    #print(stems)\n",
    "    return stems\n",
    "\n",
    "df_excel = pd.read_excel('3208.xlsx')#считываем данные в датафрейм\n",
    "\n",
    "mask = df_excel[G011_direction] == 'ИМ'#выбираем направление ИМпорт\n",
    "df = df_excel.copy()[mask]\n",
    "df = df[columns]\n",
    "df = df.groupby(G31_1_description).aggregate(sum)#группируем данные\n",
    "df = df.reset_index()\n",
    "df[G31_1_description] = df[G31_1_description].str.lower() #нижний регистр\n",
    "# начинаем отделять знаки препинания\n",
    "df[G31_1_description] = df[G31_1_description].str.replace(',', ' ') \n",
    "df[G31_1_description] = df[G31_1_description].str.replace('.', ' ') \n",
    "df[G31_1_description] = df[G31_1_description].str.replace('-', ' ')\n",
    "df[G31_1_description] = df[G31_1_description].str.replace(';', ' ')\n",
    "df[G31_1_description] = df[G31_1_description].str.replace(':', ' ')\n",
    "df[G31_1_description] = df[G31_1_description].str.replace('(', ' ')\n",
    "df[G31_1_description] = df[G31_1_description].str.replace(')', ' ')\n",
    "df[G31_1_description] = df[G31_1_description].str.replace(r'[\\W]+', ' ')\n",
    "# в тексте используеться английский и русский язык\n",
    "stemmer_rus = SnowballStemmer('russian')\n",
    "stemmer_eng = SnowballStemmer('english')\n",
    "\n",
    "#создаем стоп слова из англ и русского алфавита\n",
    "file_name_stopwords = 'stopwords-ru.txt'\n",
    "f = open(file_name_stopwords, encoding='utf8').read()\n",
    "stopwords_rus = f.split('\\n')\n",
    "stopwords_eng = nltk.corpus.stopwords.words('english')\n",
    "stopwords = []\n",
    "stopwords.extend(stopwords_rus)\n",
    "stopwords.extend(stopwords_eng)\n",
    "#векторизация\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df= 0.9, max_features= 1000, min_df=0.05, stop_words= stopwords, \n",
    "                                  use_idf= True, tokenizer=token_and_stem, ngram_range=(1,5))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df[G31_1_description])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод k-средних\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 25 #выбираем наугад число кластеров\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()\n",
    "clust = km.predict(tfidf_matrix)\n",
    "df_Kmeans = df.copy()\n",
    "df_Kmeans['Cluster'] = clusters # создаем и записываем в датафрейм кластеры\n",
    "df_Kmeans.to_excel('3208_cluster_Kmeans_25.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps = 0.3, min_samples= 10).fit(tfidf_matrix)\n",
    "labels = db.labels_.tolist()\n",
    "df_db = df.copy()\n",
    "df_db['Cluster'] = labels # создаем и записываем в датафрейм кластеры\n",
    "df_db.to_excel('3208_cluster_DBSCAN.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
