{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Исходные данные – корпус новостей с сайта Lenta.ru (https://www.kaggle.com/yutkin/corpus-of-russian-news-articles-from-lenta/). Нужно обучить классификатор новостей по рубрикам (поле topic), для чего: \n",
    "### 1.    Предобработать тексты и получить признаковое пространство\n",
    "### 2.    Выбрать модель или несколько   моделей для обучения\n",
    "### 3.    Разделить датасет на обучающую, тестовую и контрольную выборки\n",
    "### 4.    Выбрать метрику для оценки результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Т.к. наша задача сводиться к многоклассовой классификации текста по темам, то нужно перевести текст в векторной пространство и каждому тексту сопоставить номер(уникальный индентификатор) темы.\n",
    "## 2. Использовал только одну модель, т.к. при подборе параметов ч\\з \"решето\" уходит очень много вычислительного времени\n",
    "## 3.Использовал деление на обучающую и тестовую выборки в связи с тем, что использовал \"решето\" для подбора лучших параметров и кросс-валидации.\n",
    "## 4. В качестве метрики выбрал точность классификации(у нас многоклассовая классификация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer#стеминг\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer#tfidf векторизатор\n",
    "from sklearn.ensemble import RandomForestClassifier#наша модель\n",
    "from sklearn.metrics import accuracy_score #для оценки модели\n",
    "from sklearn.model_selection import GridSearchCV # для определения лучшего параметра\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Загружаем данные и обрабатываем.\n",
    "### Т.к. машина слабая, то я использую только первые 25к строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_rows = 25000\n",
    "df = pd.read_csv('lenta-ru-news.csv', nrows = number_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://lenta.ru/news/2018/12/14/cancer/</td>\n",
       "      <td>Названы регионы России с самой высокой смертно...</td>\n",
       "      <td>Вице-премьер по социальным вопросам Татьяна Го...</td>\n",
       "      <td>Россия</td>\n",
       "      <td>Общество</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://lenta.ru/news/2018/12/15/doping/</td>\n",
       "      <td>Австрия не представила доказательств вины росс...</td>\n",
       "      <td>Австрийские правоохранительные органы не предс...</td>\n",
       "      <td>Спорт</td>\n",
       "      <td>Зимние виды</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://lenta.ru/news/2018/12/15/disneyland/</td>\n",
       "      <td>Обнаружено самое счастливое место на планете</td>\n",
       "      <td>Сотрудники социальной сети Instagram проанализ...</td>\n",
       "      <td>Путешествия</td>\n",
       "      <td>Мир</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://lenta.ru/news/2018/12/15/usa25/</td>\n",
       "      <td>В США раскрыли сумму расходов на расследование...</td>\n",
       "      <td>С начала расследования российского вмешательст...</td>\n",
       "      <td>Мир</td>\n",
       "      <td>Политика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://lenta.ru/news/2018/12/15/integrity/</td>\n",
       "      <td>Хакеры рассказали о планах Великобритании зами...</td>\n",
       "      <td>Хакерская группировка Anonymous опубликовала н...</td>\n",
       "      <td>Мир</td>\n",
       "      <td>Общество</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            url  \\\n",
       "0      https://lenta.ru/news/2018/12/14/cancer/   \n",
       "1      https://lenta.ru/news/2018/12/15/doping/   \n",
       "2  https://lenta.ru/news/2018/12/15/disneyland/   \n",
       "3       https://lenta.ru/news/2018/12/15/usa25/   \n",
       "4   https://lenta.ru/news/2018/12/15/integrity/   \n",
       "\n",
       "                                               title  \\\n",
       "0  Названы регионы России с самой высокой смертно...   \n",
       "1  Австрия не представила доказательств вины росс...   \n",
       "2       Обнаружено самое счастливое место на планете   \n",
       "3  В США раскрыли сумму расходов на расследование...   \n",
       "4  Хакеры рассказали о планах Великобритании зами...   \n",
       "\n",
       "                                                text        topic         tags  \n",
       "0  Вице-премьер по социальным вопросам Татьяна Го...       Россия     Общество  \n",
       "1  Австрийские правоохранительные органы не предс...        Спорт  Зимние виды  \n",
       "2  Сотрудники социальной сети Instagram проанализ...  Путешествия          Мир  \n",
       "3  С начала расследования российского вмешательст...          Мир     Политика  \n",
       "4  Хакерская группировка Anonymous опубликовала н...          Мир     Общество  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Для упрощения работы, создаю переменные с именами столбцов к которым я буду часто обращаться\n",
    "text_col = 'text'\n",
    "topic_col = 'topic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Т.к. наша цель классификация тем(признак topic) \n",
    "#на основе текста(text), то остальные признаки не нужны\n",
    "df.drop(['url', 'title', 'tags'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      "text     25000 non-null object\n",
      "topic    24992 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 390.7+ KB\n"
     ]
    }
   ],
   "source": [
    "#смотрю пропуски\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3530</th>\n",
       "      <td>в учебном центре врачебной практики praxi medi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8736</th>\n",
       "      <td>организаторы выставки «игромир» и фестиваля co...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9956</th>\n",
       "      <td>российская студия разработчиков enplex games р...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10308</th>\n",
       "      <td>современный рынок товаров и услуг настолько ра...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16333</th>\n",
       "      <td>оргкомитет восточного экономического форума на...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17562</th>\n",
       "      <td>фонд росконгресс и федерация торгово-промышлен...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22497</th>\n",
       "      <td>сша предъявили кндр требования по 47 пунктам в...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24522</th>\n",
       "      <td>в красноярском крае задержали подозреваемого в...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text topic\n",
       "3530   в учебном центре врачебной практики praxi medi...   NaN\n",
       "8736   организаторы выставки «игромир» и фестиваля co...   NaN\n",
       "9956   российская студия разработчиков enplex games р...   NaN\n",
       "10308  современный рынок товаров и услуг настолько ра...   NaN\n",
       "16333  оргкомитет восточного экономического форума на...   NaN\n",
       "17562  фонд росконгресс и федерация торгово-промышлен...   NaN\n",
       "22497  сша предъявили кндр требования по 47 пунктам в...   NaN\n",
       "24522  в красноярском крае задержали подозреваемого в...   NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# есть текст, но нету темы. \n",
    "#Т.к. в общей совокупности данных доля пропусков очень мала, то я их удаляю\n",
    "df[df[topic_col].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 24992 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      "text     24992 non-null object\n",
      "topic    24992 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 585.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info() #пропусков нету"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Подготавливаю данные для векторизации. \n",
    "Перевожу в нижний регистр и удаляю все знаки препинания\n",
    "'''\n",
    "df[text_col] = df[text_col].str.lower()\n",
    "df[topic_col] = df[topic_col].str.lower()\n",
    "df[text_col] = df[text_col].str.replace(',', ' ')\n",
    "df[text_col] = df[text_col].str.replace('.', ' ')\n",
    "df[text_col] = df[text_col].str.replace('-', ' ')\n",
    "df[text_col] = df[text_col].str.replace(';', ' ')\n",
    "df[text_col] = df[text_col].str.replace(':', ' ')\n",
    "df[text_col] = df[text_col].str.replace('(', ' ')\n",
    "df[text_col] = df[text_col].str.replace(')', ' ')\n",
    "df[text_col] = df[text_col].str.replace(r'[\\W]+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>вице премьер по социальным вопросам татьяна го...</td>\n",
       "      <td>россия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>австрийские правоохранительные органы не предс...</td>\n",
       "      <td>спорт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>сотрудники социальной сети instagram проанализ...</td>\n",
       "      <td>путешествия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>с начала расследования российского вмешательст...</td>\n",
       "      <td>мир</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>хакерская группировка anonymous опубликовала н...</td>\n",
       "      <td>мир</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        topic\n",
       "0  вице премьер по социальным вопросам татьяна го...       россия\n",
       "1  австрийские правоохранительные органы не предс...        спорт\n",
       "2  сотрудники социальной сети instagram проанализ...  путешествия\n",
       "3  с начала расследования российского вмешательст...          мир\n",
       "4  хакерская группировка anonymous опубликовала н...          мир"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция токенизации и стемминга\n",
    "def token_and_stem(text):\n",
    "    tokens  = [word for word in nltk.word_tokenize(text)] # выделяем список слов из текста\n",
    "    #print(tokens, '\\n')\n",
    "    filt_tokens = []# очищенный список слов без цифр и которые состоят минимум из 4 символов\n",
    "    for token in tokens:\n",
    "        if re.search('[а-яА-Яa-zA-Z]', token): #убираем цифры\n",
    "            if len(token) > 3: #смотрим, что бы слова состояли минимум из 4 символов\n",
    "                filt_tokens.append(token)\n",
    "    #print(filt_tokens, '\\n')\n",
    "    stems = [] #стемминг\n",
    "    for token in filt_tokens:\n",
    "        if re.search('[а-яА-Я]', token): #для русских слов стемминг\n",
    "            stems.append(stemmer_rus.stem(token))\n",
    "        elif re.search('[a-zA-Z]', token):#для английских слов стемминг\n",
    "            stems.append(stemmer_eng.stem(token))\n",
    "    #print(stems)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаю стемминги для рус и англ языка отдельно\n",
    "stemmer_rus = SnowballStemmer('russian')\n",
    "stemmer_eng = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем стоп слова из англ и русского алфавита\n",
    "file_name_stopwords = 'stopwords-ru.txt' #нашел в интернете русские стоп-слова \n",
    "f = open(file_name_stopwords, encoding='utf8').read()\n",
    "stopwords_rus = f.split('\\n') #русские стоп-слова\n",
    "stopwords_eng = nltk.corpus.stopwords.words('english') #англ. стоп-слова\n",
    "stopwords = [] #общий список стоп-слов который объединяет русс-ие и англ-ие \n",
    "stopwords.extend(stopwords_rus)\n",
    "stopwords.extend(stopwords_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторизация\n",
    "### Использую для векторизации tf-idf метод который прописан в библиотеке. \n",
    "### Выбор данного метода мотивирован скоростью вычисления при минимальных требованиях оборудования. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'becaus', 'befor', 'could', 'doe', 'dure', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'would', 'yourselv', 'алл', 'бел', 'близк', 'бол', 'больш', 'буд', 'будеш', 'будт', 'быв', 'быва', 'быт', 'важн', 'вдал', 'ве', 'вед', 'везд', 'вернут', 'взят', 'видет', 'вмест', 'вод', 'войн', 'вообщ', 'восем', 'восемнадца', 'восемнадцат', 'восьм', 'впроч', 'врем', 'времен', 'всег', 'всегд', 'всюд', 'втор', 'выйт', 'главн', 'говор', 'голов', 'дава', 'давн', 'даж', 'далек', 'дальш', 'дар', 'дат', 'двадца', 'двадцат', 'двенадца', 'двенадцат', 'двер', 'девя', 'девят', 'девятнадца', 'девятнадцат', 'действительн', 'дела', 'ден', 'деньг', 'деся', 'десят', 'довольн', 'долг', 'долж', 'должн', 'дорог', 'дума', 'душ', 'есл', 'ест', 'ждат', 'жен', 'женщин', 'жизн', 'жит', 'зан', 'зат', 'зач', 'зде', 'земл', 'знат', 'знач', 'идт', 'имен', 'имет', 'иногд', 'кажд', 'кажет', 'каза', 'книг', 'ког', 'когд', 'комнат', 'конечн', 'котор', 'кром', 'круг', 'куд', 'лежа', 'лиц', 'лиш', 'лучш', 'люб', 'люд', 'мал', 'маленьк', 'мат', 'машин', 'межд', 'мел', 'мен', 'меньш', 'мест', 'миллион', 'мим', 'минут', 'мно', 'мног', 'многочислен', 'можн', 'можх', 'москв', 'моч', 'наверх', 'наибол', 'найт', 'нача', 'нег', 'недавн', 'недалек', 'некотор', 'нельз', 'немн', 'непрерывн', 'нередк', 'нескольк', 'нибуд', 'ниж', 'низк', 'никак', 'никогд', 'никт', 'никуд', 'нич', 'ничт', 'нов', 'ног', 'ноч', 'нужн', 'обычн', 'одиннадца', 'одиннадцат', 'одн', 'однажд', 'однак', 'оказа', 'окн', 'окол', 'опя', 'особен', 'оста', 'ответ', 'откуд', 'отовсюд', 'отсюд', 'очен', 'перв', 'писа', 'плеч', 'подойд', 'подума', 'пожалуйст', 'позж', 'пойт', 'пок', 'получ', 'помн', 'понима', 'поня', 'посл', 'последн', 'посмотрет', 'посред', 'пот', 'поч', 'почт', 'правд', 'прекрасн', 'прост', 'прот', 'процент', 'пут', 'пят', 'пятнадца', 'пятнадцат', 'работ', 'разв', 'ран', 'раньш', 'реш', 'росс', 'рук', 'русск', 'сво', 'сдела', 'сеа', 'себ', 'сегодн', 'седьм', 'сем', 'семнадца', 'семнадцат', 'сидет', 'сил', 'сказа', 'скольк', 'слишк', 'слов', 'случа', 'смотрет', 'снача', 'снов', 'соб', 'советск', 'совс', 'спасиб', 'спрос', 'сраз', 'стар', 'стат', 'сторон', 'стоя', 'стран', 'сут', 'счита', 'такж', 'тво', 'теб', 'тепер', 'тоб', 'тог', 'тогд', 'тож', 'тольк', 'трет', 'тринадца', 'тринадцат', 'туд', 'увидет', 'улиц', 'умет', 'утр', 'хорош', 'хот', 'хотел', 'хотет', 'хочеш', 'част', 'чащ', 'чег', 'четверт', 'четыр', 'четырнадца', 'четырнадцат', 'чут', 'шест', 'шестнадца', 'шестнадцат', 'эт', 'явля'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24992, 294)\n"
     ]
    }
   ],
   "source": [
    "#создаю векторизатор\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df= 0.9, max_features= 1000, min_df= 0.05, \n",
    "                                  stop_words= stopwords, use_idf= True, tokenizer=token_and_stem)\n",
    "#с помощью векторизатора перевожу в векторное пространство текст\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df[text_col]) \n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[topic_col].nunique() # смотрю кол-о уникальных тем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['россия', 'спорт', 'путешествия', 'мир', 'бывший ссср',\n",
       "       'интернет и сми', 'силовые структуры', 'экономика', 'культура',\n",
       "       'дом', 'наука и техника', 'из жизни', 'ценности', 'бизнес',\n",
       "       '69-я параллель', 'культпросвет ', 'крым'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# записываю все уникальные темы в массив topic_value\n",
    "topic_value = df[topic_col].unique()\n",
    "topic_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаю еще один признак в котором будет написан номер темы\n",
    "# таким образом каждой теме соот-т свой уникальный индентификатор\n",
    "for i, val in enumerate(topic_value):\n",
    "    mask = df[topic_col] == val\n",
    "    df.loc[mask, 'topic_class'] = int(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>вице премьер по социальным вопросам татьяна го...</td>\n",
       "      <td>россия</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>австрийские правоохранительные органы не предс...</td>\n",
       "      <td>спорт</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>сотрудники социальной сети instagram проанализ...</td>\n",
       "      <td>путешествия</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>с начала расследования российского вмешательст...</td>\n",
       "      <td>мир</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>хакерская группировка anonymous опубликовала н...</td>\n",
       "      <td>мир</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        topic  topic_class\n",
       "0  вице премьер по социальным вопросам татьяна го...       россия          0.0\n",
       "1  австрийские правоохранительные органы не предс...        спорт          1.0\n",
       "2  сотрудники социальной сети instagram проанализ...  путешествия          2.0\n",
       "3  с начала расследования российского вмешательст...          мир          3.0\n",
       "4  хакерская группировка anonymous опубликовала н...          мир          3.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# стратифицированное(по признаку \"topic_class\") деление данных на обучающую и тестовую\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, df['topic_class'], \n",
    "                                                    test_size = 0.3,\n",
    "                                                    stratify = df['topic_class'],\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17494, 294)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение модели\n",
    "### Выбор модели \"случайный лес\" обоснован тем, что у нас многоклассовая классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'n_estimators': [10, 100, 1000], 'max_depth': [5, 15, 25]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#использую \"решето\" для подбора наилучего кол-а деревьев и глубины\n",
    "r_forest = RandomForestClassifier(random_state = 42)\n",
    "num_forest = [10, 100, 1000] # возможные варианты кол-а деревьев\n",
    "num_depth = [5, 15, 25]# возможные варианты глубины деревьев\n",
    "# \"решетка\" с кросс-вал проверкой. Использую 5 фолдов для кросс проверки и использую все ядра процессора\n",
    "grid = GridSearchCV(r_forest, param_grid={'n_estimators': num_forest, 'max_depth': num_depth}, \n",
    "                    cv = 5, n_jobs= -1) \n",
    "#обучаю \"решетку\"\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка перекрестной проверки: 0.35229221447353376\n",
      "Оптимальное кол-о деревьев: 1000\n",
      "Оптимальная глубина деревьев: 25\n"
     ]
    }
   ],
   "source": [
    "cv_err = 1 - grid.best_score_ #наша кросс-валидаточная ошибка на обучающей выборке\n",
    "best_num_forest = grid.best_estimator_.n_estimators # птимальное кол-о деревьев\n",
    "best_num_depth = grid.best_estimator_.max_depth # Оптимальная глубина деревьев\n",
    "print('Ошибка перекрестной проверки:', cv_err)\n",
    "print('Оптимальное кол-о деревьев:', best_num_forest)\n",
    "print('Оптимальная глубина деревьев:', best_num_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#строю модель с оптимальными параметрами\n",
    "best_r_forest = RandomForestClassifier(n_estimators = best_num_forest, \n",
    "                                       max_depth = best_num_depth,\n",
    "                                       random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=25, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_r_forest.fit(X_train, y_train) # обучаю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = best_r_forest.predict(X_train) # классификация на обучающей выборке\n",
    "y_pred_test = best_r_forest.predict(X_test) # классификация на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность на обучающей выборке: 0.984\n",
      "Точность на тестовой выборке: 0.660\n"
     ]
    }
   ],
   "source": [
    "print('Точность на обучающей выборке: {:.3f}'.format(accuracy_score(y_train, y_pred_train)))\n",
    "print('Точность на тестовой выборке: {:.3f}'.format(accuracy_score(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Низкая точность возможна обусловлена тем, что нужно подобрать лучшие параметры для векторизации текста(токенизация, стемминг, параметры векторизатора tfidf). Можно зациклить весь этот процесс(веторизация текста и подбор моделей, параметров модели) и в каждом цикле проверять точность на валидационной выборке(При этом разделить изначально данные на обучающую, валидационную и тестовую). Затем с лучшей матрицей текста, моделью и его лучшими параметрами посторить модель и провести классификация на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
